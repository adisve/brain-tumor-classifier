{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "        self.class_names = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            image_files = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n",
    "            self.image_list.extend([os.path.join(class_folder, f) for f in image_files])\n",
    "            self.label_list.extend([label]*len(image_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = crop_img(img)\n",
    "        \n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    ADD_PIXELS = 0\n",
    "    new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.6),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './Training'\n",
    "\n",
    "dataset = BrainTumorDataset(root_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "dataset_size = len(dataset)\n",
    "[print(dataset_size)]\n",
    "\n",
    "num_train = int(train_fraction * dataset_size)\n",
    "num_validation = int(validation_fraction * dataset_size)\n",
    "num_test = int(test_fraction * dataset_size)\n",
    "print(num_train, num_validation, num_test)\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [num_train, num_validation, num_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(BrainTumorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(43264, 128)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # Dynamic calculation of the flattened size\n",
    "        x_size = x.size()[1] * x.size()[2] * x.size()[3]\n",
    "        \n",
    "        x = x.view(-1, x_size)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BrainTumorCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/399], Loss: 4.8737\n",
      "Epoch [1/20], Step [2/399], Loss: 4.8200\n",
      "Epoch [1/20], Step [3/399], Loss: 4.7527\n",
      "Epoch [1/20], Step [4/399], Loss: 4.7072\n",
      "Epoch [1/20], Step [5/399], Loss: 4.6723\n",
      "Epoch [1/20], Step [6/399], Loss: 4.6189\n",
      "Epoch [1/20], Step [7/399], Loss: 4.5869\n",
      "Epoch [1/20], Step [8/399], Loss: 4.5860\n",
      "Epoch [1/20], Step [9/399], Loss: 4.4986\n",
      "Epoch [1/20], Step [10/399], Loss: 4.4738\n",
      "Epoch [1/20], Step [11/399], Loss: 4.4043\n",
      "Epoch [1/20], Step [12/399], Loss: 4.2977\n",
      "Epoch [1/20], Step [13/399], Loss: 4.2703\n",
      "Epoch [1/20], Step [14/399], Loss: 4.2959\n",
      "Epoch [1/20], Step [15/399], Loss: 4.1875\n",
      "Epoch [1/20], Step [16/399], Loss: 4.1449\n",
      "Epoch [1/20], Step [17/399], Loss: 4.1274\n",
      "Epoch [1/20], Step [18/399], Loss: 4.0735\n",
      "Epoch [1/20], Step [19/399], Loss: 4.0290\n",
      "Epoch [1/20], Step [20/399], Loss: 3.9291\n",
      "Epoch [1/20], Step [21/399], Loss: 3.8568\n",
      "Epoch [1/20], Step [22/399], Loss: 3.8128\n",
      "Epoch [1/20], Step [23/399], Loss: 3.8395\n",
      "Epoch [1/20], Step [24/399], Loss: 3.8321\n",
      "Epoch [1/20], Step [25/399], Loss: 3.6692\n",
      "Epoch [1/20], Step [26/399], Loss: 3.6587\n",
      "Epoch [1/20], Step [27/399], Loss: 3.5195\n",
      "Epoch [1/20], Step [28/399], Loss: 3.6463\n",
      "Epoch [1/20], Step [29/399], Loss: 3.3917\n",
      "Epoch [1/20], Step [30/399], Loss: 3.4375\n",
      "Epoch [1/20], Step [31/399], Loss: 3.3213\n",
      "Epoch [1/20], Step [32/399], Loss: 3.3698\n",
      "Epoch [1/20], Step [33/399], Loss: 3.3097\n",
      "Epoch [1/20], Step [34/399], Loss: 3.2536\n",
      "Epoch [1/20], Step [35/399], Loss: 3.3409\n",
      "Epoch [1/20], Step [36/399], Loss: 3.4557\n",
      "Epoch [1/20], Step [37/399], Loss: 3.3851\n",
      "Epoch [1/20], Step [38/399], Loss: 3.3579\n",
      "Epoch [1/20], Step [39/399], Loss: 3.2166\n",
      "Epoch [1/20], Step [40/399], Loss: 3.3806\n",
      "Epoch [1/20], Step [41/399], Loss: 3.3120\n",
      "Epoch [1/20], Step [42/399], Loss: 3.2210\n",
      "Epoch [1/20], Step [43/399], Loss: 3.4365\n",
      "Epoch [1/20], Step [44/399], Loss: 3.3088\n",
      "Epoch [1/20], Step [45/399], Loss: 3.3935\n",
      "Epoch [1/20], Step [46/399], Loss: 3.3997\n",
      "Epoch [1/20], Step [47/399], Loss: 3.3840\n",
      "Epoch [1/20], Step [48/399], Loss: 3.2745\n",
      "Epoch [1/20], Step [49/399], Loss: 3.3195\n",
      "Epoch [1/20], Step [50/399], Loss: 3.2938\n",
      "Epoch [1/20], Step [51/399], Loss: 3.2975\n",
      "Epoch [1/20], Step [52/399], Loss: 3.2126\n",
      "Epoch [1/20], Step [53/399], Loss: 3.2521\n",
      "Epoch [1/20], Step [54/399], Loss: 3.2796\n",
      "Epoch [1/20], Step [55/399], Loss: 3.2459\n",
      "Epoch [1/20], Step [56/399], Loss: 3.2915\n",
      "Epoch [1/20], Step [57/399], Loss: 3.1528\n",
      "Epoch [1/20], Step [58/399], Loss: 3.2609\n",
      "Epoch [1/20], Step [59/399], Loss: 3.2297\n",
      "Epoch [1/20], Step [60/399], Loss: 3.2967\n",
      "Epoch [1/20], Step [61/399], Loss: 3.1818\n",
      "Epoch [1/20], Step [62/399], Loss: 3.2802\n",
      "Epoch [1/20], Step [63/399], Loss: 3.2387\n",
      "Epoch [1/20], Step [64/399], Loss: 3.2458\n",
      "Epoch [1/20], Step [65/399], Loss: 3.3038\n",
      "Epoch [1/20], Step [66/399], Loss: 3.2387\n",
      "Epoch [1/20], Step [67/399], Loss: 3.1513\n",
      "Epoch [1/20], Step [68/399], Loss: 3.2506\n",
      "Epoch [1/20], Step [69/399], Loss: 3.3600\n",
      "Epoch [1/20], Step [70/399], Loss: 3.2396\n",
      "Epoch [1/20], Step [71/399], Loss: 3.2817\n",
      "Epoch [1/20], Step [72/399], Loss: 3.1788\n",
      "Epoch [1/20], Step [73/399], Loss: 3.1849\n",
      "Epoch [1/20], Step [74/399], Loss: 3.2003\n",
      "Epoch [1/20], Step [75/399], Loss: 3.2372\n",
      "Epoch [1/20], Step [76/399], Loss: 3.2540\n",
      "Epoch [1/20], Step [77/399], Loss: 3.2606\n",
      "Epoch [1/20], Step [78/399], Loss: 3.2633\n",
      "Epoch [1/20], Step [79/399], Loss: 3.1695\n",
      "Epoch [1/20], Step [80/399], Loss: 3.1774\n",
      "Epoch [1/20], Step [81/399], Loss: 3.0204\n",
      "Epoch [1/20], Step [82/399], Loss: 3.2721\n",
      "Epoch [1/20], Step [83/399], Loss: 3.2160\n",
      "Epoch [1/20], Step [84/399], Loss: 3.2073\n",
      "Epoch [1/20], Step [85/399], Loss: 3.2993\n",
      "Epoch [1/20], Step [86/399], Loss: 3.2148\n",
      "Epoch [1/20], Step [87/399], Loss: 3.1446\n",
      "Epoch [1/20], Step [88/399], Loss: 3.0347\n",
      "Epoch [1/20], Step [89/399], Loss: 3.2010\n",
      "Epoch [1/20], Step [90/399], Loss: 3.0517\n",
      "Epoch [1/20], Step [91/399], Loss: 3.2032\n",
      "Epoch [1/20], Step [92/399], Loss: 3.3346\n",
      "Epoch [1/20], Step [93/399], Loss: 3.1609\n",
      "Epoch [1/20], Step [94/399], Loss: 3.3161\n",
      "Epoch [1/20], Step [95/399], Loss: 3.2311\n",
      "Epoch [1/20], Step [96/399], Loss: 3.1916\n",
      "Epoch [1/20], Step [97/399], Loss: 3.1405\n",
      "Epoch [1/20], Step [98/399], Loss: 3.2106\n",
      "Epoch [1/20], Step [99/399], Loss: 3.2453\n",
      "Epoch [1/20], Step [100/399], Loss: 3.2657\n",
      "Epoch [1/20], Step [101/399], Loss: 3.1892\n",
      "Epoch [1/20], Step [102/399], Loss: 3.1597\n",
      "Epoch [1/20], Step [103/399], Loss: 3.0858\n",
      "Epoch [1/20], Step [104/399], Loss: 3.1926\n",
      "Epoch [1/20], Step [105/399], Loss: 3.2460\n",
      "Epoch [1/20], Step [106/399], Loss: 3.2982\n",
      "Epoch [1/20], Step [107/399], Loss: 3.1489\n",
      "Epoch [1/20], Step [108/399], Loss: 3.2221\n",
      "Epoch [1/20], Step [109/399], Loss: 3.0696\n",
      "Epoch [1/20], Step [110/399], Loss: 3.3303\n",
      "Epoch [1/20], Step [111/399], Loss: 3.2266\n",
      "Epoch [1/20], Step [112/399], Loss: 3.2962\n",
      "Epoch [1/20], Step [113/399], Loss: 3.1998\n",
      "Epoch [1/20], Step [114/399], Loss: 3.2577\n",
      "Epoch [1/20], Step [115/399], Loss: 3.3143\n",
      "Epoch [1/20], Step [116/399], Loss: 3.2822\n",
      "Epoch [1/20], Step [117/399], Loss: 3.2551\n",
      "Epoch [1/20], Step [118/399], Loss: 3.2013\n",
      "Epoch [1/20], Step [119/399], Loss: 3.1088\n",
      "Epoch [1/20], Step [120/399], Loss: 3.4123\n",
      "Epoch [1/20], Step [121/399], Loss: 3.2168\n",
      "Epoch [1/20], Step [122/399], Loss: 3.2276\n",
      "Epoch [1/20], Step [123/399], Loss: 3.2103\n",
      "Epoch [1/20], Step [124/399], Loss: 3.2833\n",
      "Epoch [1/20], Step [125/399], Loss: 3.1826\n",
      "Epoch [1/20], Step [126/399], Loss: 3.1448\n",
      "Epoch [1/20], Step [127/399], Loss: 3.2319\n",
      "Epoch [1/20], Step [128/399], Loss: 3.2301\n",
      "Epoch [1/20], Step [129/399], Loss: 3.1915\n",
      "Epoch [1/20], Step [130/399], Loss: 3.1709\n",
      "Epoch [1/20], Step [131/399], Loss: 3.2622\n",
      "Epoch [1/20], Step [132/399], Loss: 3.2324\n",
      "Epoch [1/20], Step [133/399], Loss: 3.3403\n",
      "Epoch [1/20], Step [134/399], Loss: 3.2876\n",
      "Epoch [1/20], Step [135/399], Loss: 3.1956\n",
      "Epoch [1/20], Step [136/399], Loss: 3.2258\n",
      "Epoch [1/20], Step [137/399], Loss: 3.2172\n",
      "Epoch [1/20], Step [138/399], Loss: 3.2986\n",
      "Epoch [1/20], Step [139/399], Loss: 3.1417\n",
      "Epoch [1/20], Step [140/399], Loss: 3.2353\n",
      "Epoch [1/20], Step [141/399], Loss: 3.2880\n",
      "Epoch [1/20], Step [142/399], Loss: 3.1376\n",
      "Epoch [1/20], Step [143/399], Loss: 3.2433\n",
      "Epoch [1/20], Step [144/399], Loss: 3.2651\n",
      "Epoch [1/20], Step [145/399], Loss: 3.2789\n",
      "Epoch [1/20], Step [146/399], Loss: 3.2193\n",
      "Epoch [1/20], Step [147/399], Loss: 3.2951\n",
      "Epoch [1/20], Step [148/399], Loss: 3.2443\n",
      "Epoch [1/20], Step [149/399], Loss: 3.1183\n",
      "Epoch [1/20], Step [150/399], Loss: 3.2432\n",
      "Epoch [1/20], Step [151/399], Loss: 3.2530\n",
      "Epoch [1/20], Step [152/399], Loss: 3.2644\n",
      "Epoch [1/20], Step [153/399], Loss: 3.1860\n",
      "Epoch [1/20], Step [154/399], Loss: 3.3194\n",
      "Epoch [1/20], Step [155/399], Loss: 3.2672\n",
      "Epoch [1/20], Step [156/399], Loss: 3.2241\n",
      "Epoch [1/20], Step [157/399], Loss: 3.2887\n",
      "Epoch [1/20], Step [158/399], Loss: 3.1854\n",
      "Epoch [1/20], Step [159/399], Loss: 3.3352\n",
      "Epoch [1/20], Step [160/399], Loss: 3.2590\n",
      "Epoch [1/20], Step [161/399], Loss: 3.2514\n",
      "Epoch [1/20], Step [162/399], Loss: 3.2973\n",
      "Epoch [1/20], Step [163/399], Loss: 3.0171\n",
      "Epoch [1/20], Step [164/399], Loss: 3.3297\n",
      "Epoch [1/20], Step [165/399], Loss: 3.2303\n",
      "Epoch [1/20], Step [166/399], Loss: 3.1628\n",
      "Epoch [1/20], Step [167/399], Loss: 3.1911\n",
      "Epoch [1/20], Step [168/399], Loss: 3.2802\n",
      "Epoch [1/20], Step [169/399], Loss: 3.1189\n",
      "Epoch [1/20], Step [170/399], Loss: 3.2288\n",
      "Epoch [1/20], Step [171/399], Loss: 3.2252\n",
      "Epoch [1/20], Step [172/399], Loss: 3.2331\n",
      "Epoch [1/20], Step [173/399], Loss: 3.1527\n",
      "Epoch [1/20], Step [174/399], Loss: 3.1123\n",
      "Epoch [1/20], Step [175/399], Loss: 3.2603\n",
      "Epoch [1/20], Step [176/399], Loss: 3.1718\n",
      "Epoch [1/20], Step [177/399], Loss: 3.2266\n",
      "Epoch [1/20], Step [178/399], Loss: 3.1509\n",
      "Epoch [1/20], Step [179/399], Loss: 3.2194\n",
      "Epoch [1/20], Step [180/399], Loss: 3.2283\n",
      "Epoch [1/20], Step [181/399], Loss: 3.2068\n",
      "Epoch [1/20], Step [182/399], Loss: 3.2334\n",
      "Epoch [1/20], Step [183/399], Loss: 3.1261\n",
      "Epoch [1/20], Step [184/399], Loss: 3.2609\n",
      "Epoch [1/20], Step [185/399], Loss: 3.2560\n",
      "Epoch [1/20], Step [186/399], Loss: 3.1399\n",
      "Epoch [1/20], Step [187/399], Loss: 3.1758\n",
      "Epoch [1/20], Step [188/399], Loss: 3.1789\n",
      "Epoch [1/20], Step [189/399], Loss: 3.0931\n",
      "Epoch [1/20], Step [190/399], Loss: 3.1350\n",
      "Epoch [1/20], Step [191/399], Loss: 3.2495\n",
      "Epoch [1/20], Step [192/399], Loss: 3.2262\n",
      "Epoch [1/20], Step [193/399], Loss: 3.1143\n",
      "Epoch [1/20], Step [194/399], Loss: 3.2158\n",
      "Epoch [1/20], Step [195/399], Loss: 3.1306\n",
      "Epoch [1/20], Step [196/399], Loss: 3.0937\n",
      "Epoch [1/20], Step [197/399], Loss: 3.2175\n",
      "Epoch [1/20], Step [198/399], Loss: 3.2348\n",
      "Epoch [1/20], Step [199/399], Loss: 3.1688\n",
      "Epoch [1/20], Step [200/399], Loss: 3.3479\n",
      "Epoch [1/20], Step [201/399], Loss: 3.2616\n",
      "Epoch [1/20], Step [202/399], Loss: 3.1959\n",
      "Epoch [1/20], Step [203/399], Loss: 3.2171\n",
      "Epoch [1/20], Step [204/399], Loss: 3.1895\n",
      "Epoch [1/20], Step [205/399], Loss: 3.2144\n",
      "Epoch [1/20], Step [206/399], Loss: 3.3746\n",
      "Epoch [1/20], Step [207/399], Loss: 3.3444\n",
      "Epoch [1/20], Step [208/399], Loss: 3.2937\n",
      "Epoch [1/20], Step [209/399], Loss: 3.2810\n",
      "Epoch [1/20], Step [210/399], Loss: 3.1006\n",
      "Epoch [1/20], Step [211/399], Loss: 3.3257\n",
      "Epoch [1/20], Step [212/399], Loss: 3.3223\n",
      "Epoch [1/20], Step [213/399], Loss: 3.2082\n",
      "Epoch [1/20], Step [214/399], Loss: 3.2314\n",
      "Epoch [1/20], Step [215/399], Loss: 3.0876\n",
      "Epoch [1/20], Step [216/399], Loss: 3.1821\n",
      "Epoch [1/20], Step [217/399], Loss: 3.0436\n",
      "Epoch [1/20], Step [218/399], Loss: 3.1279\n",
      "Epoch [1/20], Step [219/399], Loss: 3.2011\n",
      "Epoch [1/20], Step [220/399], Loss: 3.2976\n",
      "Epoch [1/20], Step [221/399], Loss: 3.2448\n",
      "Epoch [1/20], Step [222/399], Loss: 3.2138\n",
      "Epoch [1/20], Step [223/399], Loss: 3.3531\n",
      "Epoch [1/20], Step [224/399], Loss: 3.2206\n",
      "Epoch [1/20], Step [225/399], Loss: 3.3157\n",
      "Epoch [1/20], Step [226/399], Loss: 3.2606\n",
      "Epoch [1/20], Step [227/399], Loss: 3.1697\n",
      "Epoch [1/20], Step [228/399], Loss: 3.2740\n",
      "Epoch [1/20], Step [229/399], Loss: 3.2380\n",
      "Epoch [1/20], Step [230/399], Loss: 3.1924\n",
      "Epoch [1/20], Step [231/399], Loss: 3.0618\n",
      "Epoch [1/20], Step [232/399], Loss: 3.0423\n",
      "Epoch [1/20], Step [233/399], Loss: 3.2694\n",
      "Epoch [1/20], Step [234/399], Loss: 3.1431\n",
      "Epoch [1/20], Step [235/399], Loss: 3.2973\n",
      "Epoch [1/20], Step [236/399], Loss: 3.1189\n",
      "Epoch [1/20], Step [237/399], Loss: 3.2144\n",
      "Epoch [1/20], Step [238/399], Loss: 3.2497\n",
      "Epoch [1/20], Step [239/399], Loss: 3.2661\n",
      "Epoch [1/20], Step [240/399], Loss: 3.2423\n",
      "Epoch [1/20], Step [241/399], Loss: 3.3478\n",
      "Epoch [1/20], Step [242/399], Loss: 3.2955\n",
      "Epoch [1/20], Step [243/399], Loss: 3.2141\n",
      "Epoch [1/20], Step [244/399], Loss: 3.0673\n",
      "Epoch [1/20], Step [245/399], Loss: 3.2029\n",
      "Epoch [1/20], Step [246/399], Loss: 3.2502\n",
      "Epoch [1/20], Step [247/399], Loss: 3.2094\n",
      "Epoch [1/20], Step [248/399], Loss: 3.2267\n",
      "Epoch [1/20], Step [249/399], Loss: 3.2132\n",
      "Epoch [1/20], Step [250/399], Loss: 3.2337\n",
      "Epoch [1/20], Step [251/399], Loss: 3.2375\n",
      "Epoch [1/20], Step [252/399], Loss: 3.1757\n",
      "Epoch [1/20], Step [253/399], Loss: 3.0913\n",
      "Epoch [1/20], Step [254/399], Loss: 3.1962\n",
      "Epoch [1/20], Step [255/399], Loss: 3.2345\n",
      "Epoch [1/20], Step [256/399], Loss: 3.2030\n",
      "Epoch [1/20], Step [257/399], Loss: 3.1563\n",
      "Epoch [1/20], Step [258/399], Loss: 3.1556\n",
      "Epoch [1/20], Step [259/399], Loss: 3.2720\n",
      "Epoch [1/20], Step [260/399], Loss: 3.3900\n",
      "Epoch [1/20], Step [261/399], Loss: 3.0820\n",
      "Epoch [1/20], Step [262/399], Loss: 3.3033\n",
      "Epoch [1/20], Step [263/399], Loss: 3.2083\n",
      "Epoch [1/20], Step [264/399], Loss: 3.0868\n",
      "Epoch [1/20], Step [265/399], Loss: 3.2347\n",
      "Epoch [1/20], Step [266/399], Loss: 3.2988\n",
      "Epoch [1/20], Step [267/399], Loss: 3.2804\n",
      "Epoch [1/20], Step [268/399], Loss: 3.2036\n",
      "Epoch [1/20], Step [269/399], Loss: 3.2951\n",
      "Epoch [1/20], Step [270/399], Loss: 3.1753\n",
      "Epoch [1/20], Step [271/399], Loss: 3.2297\n",
      "Epoch [1/20], Step [272/399], Loss: 3.1959\n",
      "Epoch [1/20], Step [273/399], Loss: 3.1331\n",
      "Epoch [1/20], Step [274/399], Loss: 3.1448\n",
      "Epoch [1/20], Step [275/399], Loss: 3.2513\n",
      "Epoch [1/20], Step [276/399], Loss: 3.2835\n",
      "Epoch [1/20], Step [277/399], Loss: 3.2186\n",
      "Epoch [1/20], Step [278/399], Loss: 3.1926\n",
      "Epoch [1/20], Step [279/399], Loss: 3.2479\n",
      "Epoch [1/20], Step [280/399], Loss: 3.2999\n",
      "Epoch [1/20], Step [281/399], Loss: 3.1885\n",
      "Epoch [1/20], Step [282/399], Loss: 3.2617\n",
      "Epoch [1/20], Step [283/399], Loss: 3.1605\n",
      "Epoch [1/20], Step [284/399], Loss: 3.1924\n",
      "Epoch [1/20], Step [285/399], Loss: 3.2400\n",
      "Epoch [1/20], Step [286/399], Loss: 3.1854\n",
      "Epoch [1/20], Step [287/399], Loss: 3.1853\n",
      "Epoch [1/20], Step [288/399], Loss: 3.2803\n",
      "Epoch [1/20], Step [289/399], Loss: 3.2550\n",
      "Epoch [1/20], Step [290/399], Loss: 3.2444\n",
      "Epoch [1/20], Step [291/399], Loss: 3.1483\n",
      "Epoch [1/20], Step [292/399], Loss: 3.2026\n",
      "Epoch [1/20], Step [293/399], Loss: 3.2677\n",
      "Epoch [1/20], Step [294/399], Loss: 3.1459\n",
      "Epoch [1/20], Step [295/399], Loss: 3.1708\n",
      "Epoch [1/20], Step [296/399], Loss: 3.1432\n",
      "Epoch [1/20], Step [297/399], Loss: 3.2260\n",
      "Epoch [1/20], Step [298/399], Loss: 3.2185\n",
      "Epoch [1/20], Step [299/399], Loss: 3.2710\n",
      "Epoch [1/20], Step [300/399], Loss: 3.2557\n",
      "Epoch [1/20], Step [301/399], Loss: 3.2826\n",
      "Epoch [1/20], Step [302/399], Loss: 3.3137\n",
      "Epoch [1/20], Step [303/399], Loss: 3.2392\n",
      "Epoch [1/20], Step [304/399], Loss: 3.2074\n",
      "Epoch [1/20], Step [305/399], Loss: 3.1806\n",
      "Epoch [1/20], Step [306/399], Loss: 3.2196\n",
      "Epoch [1/20], Step [307/399], Loss: 3.1670\n",
      "Epoch [1/20], Step [308/399], Loss: 3.1432\n",
      "Epoch [1/20], Step [309/399], Loss: 3.1460\n",
      "Epoch [1/20], Step [310/399], Loss: 3.1460\n",
      "Epoch [1/20], Step [311/399], Loss: 3.3188\n",
      "Epoch [1/20], Step [312/399], Loss: 3.2169\n",
      "Epoch [1/20], Step [313/399], Loss: 3.1941\n",
      "Epoch [1/20], Step [314/399], Loss: 3.1920\n",
      "Epoch [1/20], Step [315/399], Loss: 3.1715\n",
      "Epoch [1/20], Step [316/399], Loss: 3.1664\n",
      "Epoch [1/20], Step [317/399], Loss: 3.2525\n",
      "Epoch [1/20], Step [318/399], Loss: 3.2755\n",
      "Epoch [1/20], Step [319/399], Loss: 3.2266\n",
      "Epoch [1/20], Step [320/399], Loss: 3.2487\n",
      "Epoch [1/20], Step [321/399], Loss: 3.2174\n",
      "Epoch [1/20], Step [322/399], Loss: 3.1982\n",
      "Epoch [1/20], Step [323/399], Loss: 3.1771\n",
      "Epoch [1/20], Step [324/399], Loss: 3.2223\n",
      "Epoch [1/20], Step [325/399], Loss: 3.1377\n",
      "Epoch [1/20], Step [326/399], Loss: 3.3022\n",
      "Epoch [1/20], Step [327/399], Loss: 3.2373\n",
      "Epoch [1/20], Step [328/399], Loss: 3.2697\n",
      "Epoch [1/20], Step [329/399], Loss: 3.2420\n",
      "Epoch [1/20], Step [330/399], Loss: 3.1895\n",
      "Epoch [1/20], Step [331/399], Loss: 3.2501\n",
      "Epoch [1/20], Step [332/399], Loss: 3.2710\n",
      "Epoch [1/20], Step [333/399], Loss: 3.1360\n",
      "Epoch [1/20], Step [334/399], Loss: 3.0061\n",
      "Epoch [1/20], Step [335/399], Loss: 3.2458\n",
      "Epoch [1/20], Step [336/399], Loss: 3.2030\n",
      "Epoch [1/20], Step [337/399], Loss: 3.2130\n",
      "Epoch [1/20], Step [338/399], Loss: 3.3337\n",
      "Epoch [1/20], Step [339/399], Loss: 3.1783\n",
      "Epoch [1/20], Step [340/399], Loss: 3.0887\n",
      "Epoch [1/20], Step [341/399], Loss: 3.1794\n",
      "Epoch [1/20], Step [342/399], Loss: 3.1435\n",
      "Epoch [1/20], Step [343/399], Loss: 3.1511\n",
      "Epoch [1/20], Step [344/399], Loss: 3.1553\n",
      "Epoch [1/20], Step [345/399], Loss: 3.2548\n",
      "Epoch [1/20], Step [346/399], Loss: 3.2190\n",
      "Epoch [1/20], Step [347/399], Loss: 3.3451\n",
      "Epoch [1/20], Step [348/399], Loss: 3.1954\n",
      "Epoch [1/20], Step [349/399], Loss: 3.2005\n",
      "Epoch [1/20], Step [350/399], Loss: 3.2563\n",
      "Epoch [1/20], Step [351/399], Loss: 3.2605\n",
      "Epoch [1/20], Step [352/399], Loss: 3.2151\n",
      "Epoch [1/20], Step [353/399], Loss: 3.1354\n",
      "Epoch [1/20], Step [354/399], Loss: 3.1130\n",
      "Epoch [1/20], Step [355/399], Loss: 3.1771\n",
      "Epoch [1/20], Step [356/399], Loss: 3.1893\n",
      "Epoch [1/20], Step [357/399], Loss: 3.1196\n",
      "Epoch [1/20], Step [358/399], Loss: 3.3162\n",
      "Epoch [1/20], Step [359/399], Loss: 3.2476\n",
      "Epoch [1/20], Step [360/399], Loss: 3.2884\n",
      "Epoch [1/20], Step [361/399], Loss: 3.2205\n",
      "Epoch [1/20], Step [362/399], Loss: 3.1460\n",
      "Epoch [1/20], Step [363/399], Loss: 3.1190\n",
      "Epoch [1/20], Step [364/399], Loss: 3.2762\n",
      "Epoch [1/20], Step [365/399], Loss: 3.3629\n",
      "Epoch [1/20], Step [366/399], Loss: 3.3582\n",
      "Epoch [1/20], Step [367/399], Loss: 3.2400\n",
      "Epoch [1/20], Step [368/399], Loss: 3.3077\n",
      "Epoch [1/20], Step [369/399], Loss: 3.2238\n",
      "Epoch [1/20], Step [370/399], Loss: 3.2691\n",
      "Epoch [1/20], Step [371/399], Loss: 3.3018\n",
      "Epoch [1/20], Step [372/399], Loss: 3.1466\n",
      "Epoch [1/20], Step [373/399], Loss: 3.0669\n",
      "Epoch [1/20], Step [374/399], Loss: 3.2260\n",
      "Epoch [1/20], Step [375/399], Loss: 3.1629\n",
      "Epoch [1/20], Step [376/399], Loss: 3.2281\n",
      "Epoch [1/20], Step [377/399], Loss: 2.9835\n",
      "Epoch [1/20], Step [378/399], Loss: 3.2039\n",
      "Epoch [1/20], Step [379/399], Loss: 3.2497\n",
      "Epoch [1/20], Step [380/399], Loss: 3.1837\n",
      "Epoch [1/20], Step [381/399], Loss: 3.1616\n",
      "Epoch [1/20], Step [382/399], Loss: 3.2170\n",
      "Epoch [1/20], Step [383/399], Loss: 3.2404\n",
      "Epoch [1/20], Step [384/399], Loss: 3.1671\n",
      "Epoch [1/20], Step [385/399], Loss: 3.2379\n",
      "Epoch [1/20], Step [386/399], Loss: 3.2817\n",
      "Epoch [1/20], Step [387/399], Loss: 3.1273\n",
      "Epoch [1/20], Step [388/399], Loss: 3.1762\n",
      "Epoch [1/20], Step [389/399], Loss: 3.1528\n",
      "Epoch [1/20], Step [390/399], Loss: 3.2624\n",
      "Epoch [1/20], Step [391/399], Loss: 3.1805\n",
      "Epoch [1/20], Step [392/399], Loss: 3.1980\n",
      "Epoch [1/20], Step [393/399], Loss: 3.2358\n",
      "Epoch [1/20], Step [394/399], Loss: 3.2093\n",
      "Epoch [1/20], Step [395/399], Loss: 3.2513\n",
      "Epoch [1/20], Step [396/399], Loss: 3.2267\n",
      "Epoch [1/20], Step [397/399], Loss: 3.3781\n",
      "Epoch [1/20], Step [398/399], Loss: 3.2609\n",
      "Epoch [1/20], Step [399/399], Loss: 3.2401\n",
      "\n",
      "Metrics after Epoch 1 - Accuracy: 0.2882, Precision: 0.2896, Recall: 0.2882, F1: 0.2705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adisveletanlic/git/ml-project/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Step [1/399], Loss: 3.1790\n",
      "Epoch [2/20], Step [2/399], Loss: 3.2390\n",
      "Epoch [2/20], Step [3/399], Loss: 3.1888\n",
      "Epoch [2/20], Step [4/399], Loss: 3.1454\n",
      "Epoch [2/20], Step [5/399], Loss: 3.2888\n",
      "Epoch [2/20], Step [6/399], Loss: 3.1108\n",
      "Epoch [2/20], Step [7/399], Loss: 3.1808\n",
      "Epoch [2/20], Step [8/399], Loss: 3.1507\n",
      "Epoch [2/20], Step [9/399], Loss: 3.1374\n",
      "Epoch [2/20], Step [10/399], Loss: 3.3091\n",
      "Epoch [2/20], Step [11/399], Loss: 3.2492\n",
      "Epoch [2/20], Step [12/399], Loss: 3.1634\n",
      "Epoch [2/20], Step [13/399], Loss: 3.1878\n",
      "Epoch [2/20], Step [14/399], Loss: 3.1919\n",
      "Epoch [2/20], Step [15/399], Loss: 3.1641\n",
      "Epoch [2/20], Step [16/399], Loss: 3.2610\n",
      "Epoch [2/20], Step [17/399], Loss: 3.2110\n",
      "Epoch [2/20], Step [18/399], Loss: 3.2091\n",
      "Epoch [2/20], Step [19/399], Loss: 3.2159\n",
      "Epoch [2/20], Step [20/399], Loss: 3.2513\n",
      "Epoch [2/20], Step [21/399], Loss: 3.1191\n",
      "Epoch [2/20], Step [22/399], Loss: 3.2375\n",
      "Epoch [2/20], Step [23/399], Loss: 3.1717\n",
      "Epoch [2/20], Step [24/399], Loss: 3.1931\n",
      "Epoch [2/20], Step [25/399], Loss: 3.2366\n",
      "Epoch [2/20], Step [26/399], Loss: 3.1838\n",
      "Epoch [2/20], Step [27/399], Loss: 3.2333\n",
      "Epoch [2/20], Step [28/399], Loss: 3.2541\n",
      "Epoch [2/20], Step [29/399], Loss: 3.2151\n",
      "Epoch [2/20], Step [30/399], Loss: 3.1963\n",
      "Epoch [2/20], Step [31/399], Loss: 3.3432\n",
      "Epoch [2/20], Step [32/399], Loss: 3.2459\n",
      "Epoch [2/20], Step [33/399], Loss: 3.2691\n",
      "Epoch [2/20], Step [34/399], Loss: 3.3273\n",
      "Epoch [2/20], Step [35/399], Loss: 3.2790\n",
      "Epoch [2/20], Step [36/399], Loss: 3.2723\n",
      "Epoch [2/20], Step [37/399], Loss: 3.3303\n",
      "Epoch [2/20], Step [38/399], Loss: 3.1767\n",
      "Epoch [2/20], Step [39/399], Loss: 3.2841\n",
      "Epoch [2/20], Step [40/399], Loss: 3.2560\n",
      "Epoch [2/20], Step [41/399], Loss: 3.1806\n",
      "Epoch [2/20], Step [42/399], Loss: 3.2390\n",
      "Epoch [2/20], Step [43/399], Loss: 3.0688\n",
      "Epoch [2/20], Step [44/399], Loss: 3.1616\n",
      "Epoch [2/20], Step [45/399], Loss: 3.3047\n",
      "Epoch [2/20], Step [46/399], Loss: 3.2830\n",
      "Epoch [2/20], Step [47/399], Loss: 3.2773\n",
      "Epoch [2/20], Step [48/399], Loss: 3.2199\n",
      "Epoch [2/20], Step [49/399], Loss: 3.1161\n",
      "Epoch [2/20], Step [50/399], Loss: 3.2584\n",
      "Epoch [2/20], Step [51/399], Loss: 3.3635\n",
      "Epoch [2/20], Step [52/399], Loss: 3.0156\n",
      "Epoch [2/20], Step [53/399], Loss: 3.1951\n",
      "Epoch [2/20], Step [54/399], Loss: 3.2763\n",
      "Epoch [2/20], Step [55/399], Loss: 3.2009\n",
      "Epoch [2/20], Step [56/399], Loss: 3.2011\n",
      "Epoch [2/20], Step [57/399], Loss: 3.1174\n",
      "Epoch [2/20], Step [58/399], Loss: 3.2576\n",
      "Epoch [2/20], Step [59/399], Loss: 3.2673\n",
      "Epoch [2/20], Step [60/399], Loss: 3.2417\n",
      "Epoch [2/20], Step [61/399], Loss: 3.1265\n",
      "Epoch [2/20], Step [62/399], Loss: 3.2111\n",
      "Epoch [2/20], Step [63/399], Loss: 3.2353\n",
      "Epoch [2/20], Step [64/399], Loss: 3.1196\n",
      "Epoch [2/20], Step [65/399], Loss: 3.2774\n",
      "Epoch [2/20], Step [66/399], Loss: 3.1901\n",
      "Epoch [2/20], Step [67/399], Loss: 3.2475\n",
      "Epoch [2/20], Step [68/399], Loss: 3.2644\n",
      "Epoch [2/20], Step [69/399], Loss: 3.2409\n",
      "Epoch [2/20], Step [70/399], Loss: 3.1470\n",
      "Epoch [2/20], Step [71/399], Loss: 3.0905\n",
      "Epoch [2/20], Step [72/399], Loss: 3.2047\n",
      "Epoch [2/20], Step [73/399], Loss: 3.1879\n",
      "Epoch [2/20], Step [74/399], Loss: 3.3344\n",
      "Epoch [2/20], Step [75/399], Loss: 3.3918\n",
      "Epoch [2/20], Step [76/399], Loss: 3.2045\n",
      "Epoch [2/20], Step [77/399], Loss: 3.3035\n",
      "Epoch [2/20], Step [78/399], Loss: 3.2701\n",
      "Epoch [2/20], Step [79/399], Loss: 3.0938\n",
      "Epoch [2/20], Step [80/399], Loss: 3.1501\n",
      "Epoch [2/20], Step [81/399], Loss: 3.1651\n",
      "Epoch [2/20], Step [82/399], Loss: 3.1822\n",
      "Epoch [2/20], Step [83/399], Loss: 3.1818\n",
      "Epoch [2/20], Step [84/399], Loss: 3.2051\n",
      "Epoch [2/20], Step [85/399], Loss: 3.2637\n",
      "Epoch [2/20], Step [86/399], Loss: 3.2363\n",
      "Epoch [2/20], Step [87/399], Loss: 3.2703\n",
      "Epoch [2/20], Step [88/399], Loss: 3.3264\n",
      "Epoch [2/20], Step [89/399], Loss: 3.2161\n",
      "Epoch [2/20], Step [90/399], Loss: 3.2333\n",
      "Epoch [2/20], Step [91/399], Loss: 3.0494\n",
      "Epoch [2/20], Step [92/399], Loss: 3.1978\n",
      "Epoch [2/20], Step [93/399], Loss: 3.2317\n",
      "Epoch [2/20], Step [94/399], Loss: 3.2627\n",
      "Epoch [2/20], Step [95/399], Loss: 3.1997\n",
      "Epoch [2/20], Step [96/399], Loss: 3.1860\n",
      "Epoch [2/20], Step [97/399], Loss: 3.0892\n",
      "Epoch [2/20], Step [98/399], Loss: 3.2309\n",
      "Epoch [2/20], Step [99/399], Loss: 3.1963\n",
      "Epoch [2/20], Step [100/399], Loss: 3.2876\n",
      "Epoch [2/20], Step [101/399], Loss: 3.1686\n",
      "Epoch [2/20], Step [102/399], Loss: 3.2299\n",
      "Epoch [2/20], Step [103/399], Loss: 3.2559\n",
      "Epoch [2/20], Step [104/399], Loss: 3.2090\n",
      "Epoch [2/20], Step [105/399], Loss: 3.3082\n",
      "Epoch [2/20], Step [106/399], Loss: 3.1026\n",
      "Epoch [2/20], Step [107/399], Loss: 3.2613\n",
      "Epoch [2/20], Step [108/399], Loss: 3.2059\n",
      "Epoch [2/20], Step [109/399], Loss: 3.2467\n",
      "Epoch [2/20], Step [110/399], Loss: 3.0700\n",
      "Epoch [2/20], Step [111/399], Loss: 3.2631\n",
      "Epoch [2/20], Step [112/399], Loss: 3.2620\n",
      "Epoch [2/20], Step [113/399], Loss: 3.2825\n",
      "Epoch [2/20], Step [114/399], Loss: 3.1919\n",
      "Epoch [2/20], Step [115/399], Loss: 3.2507\n",
      "Epoch [2/20], Step [116/399], Loss: 3.1645\n",
      "Epoch [2/20], Step [117/399], Loss: 3.1943\n",
      "Epoch [2/20], Step [118/399], Loss: 3.2179\n",
      "Epoch [2/20], Step [119/399], Loss: 3.2421\n",
      "Epoch [2/20], Step [120/399], Loss: 3.2168\n",
      "Epoch [2/20], Step [121/399], Loss: 3.1383\n",
      "Epoch [2/20], Step [122/399], Loss: 3.2146\n",
      "Epoch [2/20], Step [123/399], Loss: 3.3113\n",
      "Epoch [2/20], Step [124/399], Loss: 3.2095\n",
      "Epoch [2/20], Step [125/399], Loss: 3.2782\n",
      "Epoch [2/20], Step [126/399], Loss: 3.3128\n",
      "Epoch [2/20], Step [127/399], Loss: 3.2831\n",
      "Epoch [2/20], Step [128/399], Loss: 3.1924\n",
      "Epoch [2/20], Step [129/399], Loss: 3.2084\n",
      "Epoch [2/20], Step [130/399], Loss: 3.2110\n",
      "Epoch [2/20], Step [131/399], Loss: 3.0932\n",
      "Epoch [2/20], Step [132/399], Loss: 3.1675\n",
      "Epoch [2/20], Step [133/399], Loss: 3.2219\n",
      "Epoch [2/20], Step [134/399], Loss: 3.2247\n",
      "Epoch [2/20], Step [135/399], Loss: 3.1145\n",
      "Epoch [2/20], Step [136/399], Loss: 3.2466\n",
      "Epoch [2/20], Step [137/399], Loss: 3.1955\n",
      "Epoch [2/20], Step [138/399], Loss: 3.2011\n",
      "Epoch [2/20], Step [139/399], Loss: 3.2207\n",
      "Epoch [2/20], Step [140/399], Loss: 3.1448\n",
      "Epoch [2/20], Step [141/399], Loss: 3.2615\n",
      "Epoch [2/20], Step [142/399], Loss: 3.2387\n",
      "Epoch [2/20], Step [143/399], Loss: 3.1548\n",
      "Epoch [2/20], Step [144/399], Loss: 3.1650\n",
      "Epoch [2/20], Step [145/399], Loss: 3.2688\n",
      "Epoch [2/20], Step [146/399], Loss: 3.2494\n",
      "Epoch [2/20], Step [147/399], Loss: 3.1983\n",
      "Epoch [2/20], Step [148/399], Loss: 3.2495\n",
      "Epoch [2/20], Step [149/399], Loss: 3.2316\n",
      "Epoch [2/20], Step [150/399], Loss: 3.1149\n",
      "Epoch [2/20], Step [151/399], Loss: 3.1951\n",
      "Epoch [2/20], Step [152/399], Loss: 3.2589\n",
      "Epoch [2/20], Step [153/399], Loss: 3.1424\n",
      "Epoch [2/20], Step [154/399], Loss: 3.2312\n",
      "Epoch [2/20], Step [155/399], Loss: 3.0791\n",
      "Epoch [2/20], Step [156/399], Loss: 3.1498\n",
      "Epoch [2/20], Step [157/399], Loss: 3.2033\n",
      "Epoch [2/20], Step [158/399], Loss: 3.2690\n",
      "Epoch [2/20], Step [159/399], Loss: 3.2239\n",
      "Epoch [2/20], Step [160/399], Loss: 3.2559\n",
      "Epoch [2/20], Step [161/399], Loss: 3.1443\n",
      "Epoch [2/20], Step [162/399], Loss: 3.2537\n",
      "Epoch [2/20], Step [163/399], Loss: 3.2097\n",
      "Epoch [2/20], Step [164/399], Loss: 3.2373\n",
      "Epoch [2/20], Step [165/399], Loss: 3.1841\n",
      "Epoch [2/20], Step [166/399], Loss: 3.1143\n",
      "Epoch [2/20], Step [167/399], Loss: 3.2180\n",
      "Epoch [2/20], Step [168/399], Loss: 3.2755\n",
      "Epoch [2/20], Step [169/399], Loss: 3.1946\n",
      "Epoch [2/20], Step [170/399], Loss: 3.3450\n",
      "Epoch [2/20], Step [171/399], Loss: 3.1490\n",
      "Epoch [2/20], Step [172/399], Loss: 3.1359\n",
      "Epoch [2/20], Step [173/399], Loss: 3.2182\n",
      "Epoch [2/20], Step [174/399], Loss: 3.2575\n",
      "Epoch [2/20], Step [175/399], Loss: 3.2363\n",
      "Epoch [2/20], Step [176/399], Loss: 3.1014\n",
      "Epoch [2/20], Step [177/399], Loss: 3.2475\n",
      "Epoch [2/20], Step [178/399], Loss: 3.0271\n",
      "Epoch [2/20], Step [179/399], Loss: 3.1970\n",
      "Epoch [2/20], Step [180/399], Loss: 3.2341\n",
      "Epoch [2/20], Step [181/399], Loss: 3.0432\n",
      "Epoch [2/20], Step [182/399], Loss: 3.2064\n",
      "Epoch [2/20], Step [183/399], Loss: 3.1228\n",
      "Epoch [2/20], Step [184/399], Loss: 3.3358\n",
      "Epoch [2/20], Step [185/399], Loss: 3.2187\n",
      "Epoch [2/20], Step [186/399], Loss: 3.2476\n",
      "Epoch [2/20], Step [187/399], Loss: 3.3389\n",
      "Epoch [2/20], Step [188/399], Loss: 3.3171\n",
      "Epoch [2/20], Step [189/399], Loss: 3.1915\n",
      "Epoch [2/20], Step [190/399], Loss: 3.3190\n",
      "Epoch [2/20], Step [191/399], Loss: 3.1325\n",
      "Epoch [2/20], Step [192/399], Loss: 3.2147\n",
      "Epoch [2/20], Step [193/399], Loss: 3.2396\n",
      "Epoch [2/20], Step [194/399], Loss: 3.0997\n",
      "Epoch [2/20], Step [195/399], Loss: 3.3114\n",
      "Epoch [2/20], Step [196/399], Loss: 3.0147\n",
      "Epoch [2/20], Step [197/399], Loss: 3.2051\n",
      "Epoch [2/20], Step [198/399], Loss: 3.2059\n",
      "Epoch [2/20], Step [199/399], Loss: 3.2489\n",
      "Epoch [2/20], Step [200/399], Loss: 3.1700\n",
      "Epoch [2/20], Step [201/399], Loss: 3.1491\n",
      "Epoch [2/20], Step [202/399], Loss: 3.2392\n",
      "Epoch [2/20], Step [203/399], Loss: 3.2314\n",
      "Epoch [2/20], Step [204/399], Loss: 3.1337\n",
      "Epoch [2/20], Step [205/399], Loss: 3.2477\n",
      "Epoch [2/20], Step [206/399], Loss: 3.1567\n",
      "Epoch [2/20], Step [207/399], Loss: 3.2277\n",
      "Epoch [2/20], Step [208/399], Loss: 3.2030\n",
      "Epoch [2/20], Step [209/399], Loss: 3.1870\n",
      "Epoch [2/20], Step [210/399], Loss: 3.1006\n",
      "Epoch [2/20], Step [211/399], Loss: 3.1222\n",
      "Epoch [2/20], Step [212/399], Loss: 3.0891\n",
      "Epoch [2/20], Step [213/399], Loss: 3.2153\n",
      "Epoch [2/20], Step [214/399], Loss: 3.2915\n",
      "Epoch [2/20], Step [215/399], Loss: 3.1577\n",
      "Epoch [2/20], Step [216/399], Loss: 3.3522\n",
      "Epoch [2/20], Step [217/399], Loss: 3.1563\n",
      "Epoch [2/20], Step [218/399], Loss: 3.1253\n",
      "Epoch [2/20], Step [219/399], Loss: 3.1833\n",
      "Epoch [2/20], Step [220/399], Loss: 3.3325\n",
      "Epoch [2/20], Step [221/399], Loss: 3.1527\n",
      "Epoch [2/20], Step [222/399], Loss: 3.2892\n",
      "Epoch [2/20], Step [223/399], Loss: 3.2612\n",
      "Epoch [2/20], Step [224/399], Loss: 3.1698\n",
      "Epoch [2/20], Step [225/399], Loss: 3.1871\n",
      "Epoch [2/20], Step [226/399], Loss: 3.2919\n",
      "Epoch [2/20], Step [227/399], Loss: 3.1470\n",
      "Epoch [2/20], Step [228/399], Loss: 3.2157\n",
      "Epoch [2/20], Step [229/399], Loss: 3.1935\n",
      "Epoch [2/20], Step [230/399], Loss: 3.1912\n",
      "Epoch [2/20], Step [231/399], Loss: 3.3200\n",
      "Epoch [2/20], Step [232/399], Loss: 3.2699\n",
      "Epoch [2/20], Step [233/399], Loss: 3.1961\n",
      "Epoch [2/20], Step [234/399], Loss: 3.2010\n",
      "Epoch [2/20], Step [235/399], Loss: 3.3262\n",
      "Epoch [2/20], Step [236/399], Loss: 3.1599\n",
      "Epoch [2/20], Step [237/399], Loss: 3.2271\n",
      "Epoch [2/20], Step [238/399], Loss: 3.1556\n",
      "Epoch [2/20], Step [239/399], Loss: 3.1354\n",
      "Epoch [2/20], Step [240/399], Loss: 3.2985\n",
      "Epoch [2/20], Step [241/399], Loss: 3.2390\n",
      "Epoch [2/20], Step [242/399], Loss: 3.1815\n",
      "Epoch [2/20], Step [243/399], Loss: 3.1207\n",
      "Epoch [2/20], Step [244/399], Loss: 3.1799\n",
      "Epoch [2/20], Step [245/399], Loss: 3.1695\n",
      "Epoch [2/20], Step [246/399], Loss: 3.1361\n",
      "Epoch [2/20], Step [247/399], Loss: 3.3339\n",
      "Epoch [2/20], Step [248/399], Loss: 3.1127\n",
      "Epoch [2/20], Step [249/399], Loss: 3.2795\n",
      "Epoch [2/20], Step [250/399], Loss: 3.1657\n",
      "Epoch [2/20], Step [251/399], Loss: 3.1428\n",
      "Epoch [2/20], Step [252/399], Loss: 3.2823\n",
      "Epoch [2/20], Step [253/399], Loss: 3.1592\n",
      "Epoch [2/20], Step [254/399], Loss: 3.1721\n",
      "Epoch [2/20], Step [255/399], Loss: 3.1594\n",
      "Epoch [2/20], Step [256/399], Loss: 3.3565\n",
      "Epoch [2/20], Step [257/399], Loss: 3.2388\n",
      "Epoch [2/20], Step [258/399], Loss: 3.1628\n",
      "Epoch [2/20], Step [259/399], Loss: 3.0940\n",
      "Epoch [2/20], Step [260/399], Loss: 3.1262\n",
      "Epoch [2/20], Step [261/399], Loss: 3.1393\n",
      "Epoch [2/20], Step [262/399], Loss: 3.1791\n",
      "Epoch [2/20], Step [263/399], Loss: 3.2950\n",
      "Epoch [2/20], Step [264/399], Loss: 3.2551\n",
      "Epoch [2/20], Step [265/399], Loss: 3.1917\n",
      "Epoch [2/20], Step [266/399], Loss: 3.1312\n",
      "Epoch [2/20], Step [267/399], Loss: 3.2503\n",
      "Epoch [2/20], Step [268/399], Loss: 3.1054\n",
      "Epoch [2/20], Step [269/399], Loss: 3.2369\n",
      "Epoch [2/20], Step [270/399], Loss: 3.1239\n",
      "Epoch [2/20], Step [271/399], Loss: 3.2801\n",
      "Epoch [2/20], Step [272/399], Loss: 3.1812\n",
      "Epoch [2/20], Step [273/399], Loss: 3.3585\n",
      "Epoch [2/20], Step [274/399], Loss: 3.1630\n",
      "Epoch [2/20], Step [275/399], Loss: 3.1724\n",
      "Epoch [2/20], Step [276/399], Loss: 3.1809\n",
      "Epoch [2/20], Step [277/399], Loss: 3.1966\n",
      "Epoch [2/20], Step [278/399], Loss: 3.2703\n",
      "Epoch [2/20], Step [279/399], Loss: 3.2121\n",
      "Epoch [2/20], Step [280/399], Loss: 3.0959\n",
      "Epoch [2/20], Step [281/399], Loss: 3.2548\n",
      "Epoch [2/20], Step [282/399], Loss: 3.3732\n",
      "Epoch [2/20], Step [283/399], Loss: 3.2136\n",
      "Epoch [2/20], Step [284/399], Loss: 3.1843\n",
      "Epoch [2/20], Step [285/399], Loss: 3.2356\n",
      "Epoch [2/20], Step [286/399], Loss: 3.2552\n",
      "Epoch [2/20], Step [287/399], Loss: 3.3138\n",
      "Epoch [2/20], Step [288/399], Loss: 3.1830\n",
      "Epoch [2/20], Step [289/399], Loss: 3.3433\n",
      "Epoch [2/20], Step [290/399], Loss: 3.1969\n",
      "Epoch [2/20], Step [291/399], Loss: 3.2675\n",
      "Epoch [2/20], Step [292/399], Loss: 3.1643\n",
      "Epoch [2/20], Step [293/399], Loss: 3.2462\n",
      "Epoch [2/20], Step [294/399], Loss: 3.1925\n",
      "Epoch [2/20], Step [295/399], Loss: 3.0852\n",
      "Epoch [2/20], Step [296/399], Loss: 3.2608\n",
      "Epoch [2/20], Step [297/399], Loss: 3.3251\n",
      "Epoch [2/20], Step [298/399], Loss: 3.3111\n",
      "Epoch [2/20], Step [299/399], Loss: 3.1737\n",
      "Epoch [2/20], Step [300/399], Loss: 3.2241\n",
      "Epoch [2/20], Step [301/399], Loss: 3.2470\n",
      "Epoch [2/20], Step [302/399], Loss: 3.0934\n",
      "Epoch [2/20], Step [303/399], Loss: 3.2249\n",
      "Epoch [2/20], Step [304/399], Loss: 3.2487\n",
      "Epoch [2/20], Step [305/399], Loss: 3.1401\n",
      "Epoch [2/20], Step [306/399], Loss: 3.2154\n",
      "Epoch [2/20], Step [307/399], Loss: 3.2047\n",
      "Epoch [2/20], Step [308/399], Loss: 3.2542\n",
      "Epoch [2/20], Step [309/399], Loss: 3.1653\n",
      "Epoch [2/20], Step [310/399], Loss: 3.1108\n",
      "Epoch [2/20], Step [311/399], Loss: 3.1071\n",
      "Epoch [2/20], Step [312/399], Loss: 3.3333\n",
      "Epoch [2/20], Step [313/399], Loss: 3.2536\n",
      "Epoch [2/20], Step [314/399], Loss: 3.2511\n",
      "Epoch [2/20], Step [315/399], Loss: 3.2060\n",
      "Epoch [2/20], Step [316/399], Loss: 3.1009\n",
      "Epoch [2/20], Step [317/399], Loss: 3.1784\n",
      "Epoch [2/20], Step [318/399], Loss: 3.2362\n",
      "Epoch [2/20], Step [319/399], Loss: 3.1615\n",
      "Epoch [2/20], Step [320/399], Loss: 3.1545\n",
      "Epoch [2/20], Step [321/399], Loss: 3.1892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adisveletanlic/git/ml-project/main.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adisveletanlic/git/ml-project/main.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adisveletanlic/git/ml-project/main.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adisveletanlic/git/ml-project/main.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adisveletanlic/git/ml-project/main.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adisveletanlic/git/ml-project/main.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/git/ml-project/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/git/ml-project/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Calculate metrics\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Metrics after each epoch\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'\\nMetrics after Epoch {epoch + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir = './Testing'\n",
    "test_dataset = BrainTumorDataset(test_root_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop for evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Training/',\n",
    "    target_size=(150, 150),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'Testing/',\n",
    "    target_size=(150, 150),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    seed=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# 2nd layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# 3rd layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# 4th layer\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten and fully connect Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_generator.classes\n",
    "unique_classes = np.unique(y_train)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train)\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1, min_lr=0.001)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "points = {\n",
    "    'accuracy': history.history['accuracy'],\n",
    "    'val_accuracy': history.history['val_accuracy'],\n",
    "    'loss': history.history['loss'],\n",
    "    'val_loss': history.history['val_loss'],\n",
    "    'AUC': history.history['auc'],\n",
    "    'val_AUC': history.history['val_auc'],\n",
    "}\n",
    "\n",
    "# Plotting accuracy\n",
    "axs[0].plot(points['accuracy'])\n",
    "axs[0].plot(points['val_accuracy'])\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plotting loss\n",
    "axs[1].plot(points['loss'])\n",
    "axs[1].plot(points['val_loss'])\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plotting AUC\n",
    "axs[2].plot(points['AUC'])\n",
    "axs[2].plot(points['val_AUC'])\n",
    "axs[2].set_title('Model AUC')\n",
    "axs[2].set_ylabel('AUC')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

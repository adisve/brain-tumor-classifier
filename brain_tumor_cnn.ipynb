{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import resnet18\n","from torchvision.transforms import functional as F\n","from torchvision import transforms\n","from skimage.io import imsave\n","from skimage.io import imread\n","from copy import deepcopy\n","from tqdm import tqdm\n","from torch import nn\n","import pandas as pd\n","import numpy as np\n","import torch\n","import random\n","import pydicom\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_DIR = 'Training/'\n","IMG_SIZE = 128\n","TRAIN_BATCHSIZE = 200\n","EVAL_BATCHSIZE = 10\n","EPOCHS = 10\n","TRAIN_FRACTION = 0.8\n","TEST_FRACTION = 0.1\n","VALIDATION_FRACTION = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class BrainTumorDataset(Dataset):\n","    def __init__(self, transform=None):\n","        self.DATA_DIR = DATA_DIR\n","        self.IMG_SIZE = IMG_SIZE\n","        self.transform = transform\n","        \n","        self.labels = None\n","        self.create_labels()\n","\n","    # Create labels for each image\n","    def create_labels(self):\n","        labels = []\n","        for target, target_label in enumerate(['glioma', 'meningioma', 'no_tumor', 'pituitary']):\n","            case_dir = os.path.join(self.DATA_DIR, target_label)\n","            for fname in os.listdir(case_dir):\n","                fpath = os.path.join(case_dir, fname)\n","                labels.append((fpath, target))\n","        self.labels = labels\n","\n","    # Normalize image to 0-255 range         \n","    def normalize(self, img):\n","        img = img.astype(np.float_) * 255. / img.max()\n","        img = img.astype(np.uint8)\n","        return img\n","\n","    # Returns data with its label \n","    def __getitem__(self, idx):\n","        fpath, target = self.labels[idx]\n","        \n","        # Step 1: Check the Input Types\n","        img_arr = imread(fpath, as_gray=True)\n","        \n","        img_arr = self.normalize(img_arr)\n","        \n","        # Convert ndarray to tensor\n","        data = torch.from_numpy(img_arr)\n","        \n","        data = data.type(torch.FloatTensor)\n","        data = torch.unsqueeze(data, 0)  # add channel dimension\n","        \n","        if self.transform:\n","            data = self.transform(data)\n","        \n","        return data, target\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.RandomRotation(10),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.RandomPerspective(distortion_scale=0.2, p=0.5, interpolation=3),\n","    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n","    transforms.ToTensor(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = BrainTumorDataset(transform=train_transform)\n","print(f'Amount of data in dataset: {len(dataset)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n","    dataset, \n","    [TRAIN_FRACTION, TEST_FRACTION, VALIDATION_FRACTION]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Running on {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(\n","    train_dataset, \n","    batch_size=TRAIN_BATCHSIZE,\n","    shuffle=True\n",")\n","\n","validation_loader = DataLoader(\n","    validation_dataset, \n","    batch_size=EVAL_BATCHSIZE\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset, \n","    batch_size=EVAL_BATCHSIZE\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","net = resnet18()\n","net.conv1 = nn.Conv2d(\n","    1, \n","    64, \n","    kernel_size=(7, 7), \n","    stride=(2, 2), padding=(3, 3), bias=False\n",")\n","\n","net = net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","error_minimizer = torch.optim.SGD(net.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net_final = deepcopy(net)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_validation_accuracy = 0. \n","train_accs = []\n","val_accs = []\n","\n","for epoch in range(EPOCHS):\n","\tnet.train()\n","\n","\tprint(f\"# Epoch {epoch + 1}:\")\n","\n","\ttotal_train_examples = 0\n","\tnum_correct_train = 0\n","\n","\tfor batch_index, (inputs, targets) in tqdm(enumerate(train_loader), total=len(train_dataset)//TRAIN_BATCHSIZE):\n","\t\tinputs = inputs.to(device)\n","\t\ttargets = targets.to(device)\n","\n","\t\terror_minimizer.zero_grad()\n","\n","\t\tpredictions = net(inputs)\n","\n","\t\tloss = criterion(predictions, targets)\n","\t\tloss.backward()\n","\n","\t\terror_minimizer.step()\n","\n","\t\t_, predicted_class = predictions.max(1)\n","\t\ttotal_train_examples += predicted_class.size(0)\n","\t\tnum_correct_train += predicted_class.eq(targets).sum().item()\n","\n","\ttrain_acc = num_correct_train / total_train_examples\n","\tprint(f\"Training accuracy: {train_acc}\")\n","\ttrain_accs.append(train_acc)\n","\n","\ttotal_val_examples = 0\n","\tnum_correct_val = 0\n","\n","\tnet.eval()\n","\n","\twith torch.no_grad():\n","\t\tfor batch_index, (inputs, targets) in tqdm(enumerate(validation_loader), total=len(validation_dataset)//eval_batchsize):\n","\t\t\tinputs = inputs.to(device)\n","\t\t\ttargets = targets.to(device)\n","\t\t\tpredictions = net(inputs)\n","\n","\t\t\t_, predicted_class = predictions.max(1)\n","\t\t\ttotal_val_examples += predicted_class.size(0)\n","\t\t\tnum_correct_val += predicted_class.eq(targets).sum().item()\n","\n","\tval_acc = num_correct_val / total_val_examples\n","\tprint(f\"Validation accuracy: {val_acc}\")\n","\tval_accs.append(val_acc)\n","\n","\tif val_acc > best_validation_accuracy:\n","\t\tbest_validation_accuracy = val_acc\n","\t\tprint(\"Validation accuracy was improved. Saving new model.\")\n","\t\tnet_final = deepcopy(net)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","epochs_list = list(range(EPOCHS))\n","\n","plt.figure()\n","plt.plot(epochs_list, train_accs, 'b-', label='training set accuracy')\n","plt.plot(epochs_list, val_accs, 'r-', label='validation set accuracy')\n","plt.xlabel('epoch')\n","plt.ylabel('prediction accuracy')\n","plt.ylim(0.5, 1)\n","plt.title('Classifier training evolution:\\nprediction accuracy over time')\n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":2}
